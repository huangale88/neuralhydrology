# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
<<<<<<< HEAD
experiment_name: MFLSTM_hourly_us

# input and output frequencies.
# IMPORTANT: The order here (1D then 1h) determines the concatenation order in your MFLSTM's forward pass.
# This aligns with the common practice of processing lower frequencies first in multi-frequency models
# and matches the likely processing order of your original MFLSTM's `custom_freq_processing`.
use_frequencies:
  - 1D
  - 1h

# files to specify training, validation and test basins
=======
experiment_name: test_run

# input and output frequencies
use_frequencies:
  - 1h
  - 1D

# files to specify training, validation and test basins (relative to code root or absolute path)
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
train_basin_file: 1_basin.txt
validation_basin_file: 1_basin.txt
test_basin_file: 1_basin.txt

<<<<<<< HEAD
# training, validation and test time periods
# Time information removed from date strings to match neuralhydrology's expected format (DD/MM/YYYY)
train_start_date: "01/10/1990"
train_end_date: "30/09/2003"
validation_start_date: "01/10/2003"
validation_end_date: "30/09/2008"
test_start_date: "01/10/2008"
test_end_date: "30/09/2018"

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu, mps or None]
device: cpu

# Set global random seed
seed: 110

# Activate dev_mode to allow unrecognized configuration keys.
# This is useful when using custom models that might have their own specific parameters.
dev_mode: True
=======
# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: "01/10/1999"
train_end_date: "30/09/2008"
validation_start_date: "01/10/1996"
validation_end_date: "30/09/1999"
test_start_date: "01/10/1989"
test_end_date: "30/09/1996"

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu, mps or None]
device: cuda:0
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
<<<<<<< HEAD
validate_every: 4

# specify how many random basins to use for validation (-1 means validate on all basins)
validate_n_random_basins: -1

# specify which metrics to calculate during validation
=======
validate_every: 5

# specify how many random basins to use for validation
validate_n_random_basins: 1

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
metrics:
  - NSE

# --- Model configuration --------------------------------------------------------------------------

<<<<<<< HEAD
# base model type (this points to your custom MFLSTM class)
model: mflstm

# prediction head
head: regression
output_activation: linear

# Number of cell states of the LSTM
hidden_size: 128
=======
# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: mflstm

# prediction head [regression]. Define the head specific parameters below
head: regression

# ----> Regression settings <----
output_activation: linear

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 20
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

<<<<<<< HEAD
# Removed MFLSTM-specific parameters for dynamic input embeddings
# dynamic_embeddings: True
# dynamic_input_size_embedding:
#   1D: 5
#   1h: 16
# n_channels_dynamic_embedding: 64

# Removed n_layers as it's not in the provided documentation
# n_layers: 1

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer
optimizer: Adam

# specify loss
loss: MSE

# specify learning rates to use starting at specific epochs
learning_rate:
  0: 5e-4
  10: 1e-4
  25: 1e-5
=======
# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: MSE

# specify regularization
regularization:
  - tie_frequencies

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 1e-2
  30: 5e-3
  40: 1e-3
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f

# Mini-batch size
batch_size: 256

<<<<<<< HEAD
# Batch size for evaluation
batch_size_evaluation: 1024

# Number of training epochs
epochs: 30
=======
# Number of training epochs
epochs: 50
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
<<<<<<< HEAD
# This must be a dictionary with one key per frequency, even if only one frequency is used for targets.
predict_last_n:
  1D: 1  # Placeholder for daily frequency, as the dataset expects a value.
  1h: 24 # Predict the last 24 hours for the '1h' target.

# Length of the input sequence per frequency.
# Your MFLSTM internally concatenates these for the total sequence length.
seq_length:
  1D: 365
  1h: 8760 # (365 * 24)
=======
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n:
  1D: 1
  1h: 24

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length:
  1D: 365
  1h: 336
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 5

# If true, writes logging results into tensorboard file
log_tensorboard: False

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 0

# Save model weights every n epochs
save_weights_every: 1

<<<<<<< HEAD
# Store the results of the validation to disk
save_validation_results: True

# --- Data configurations --------------------------------------------------------------------------

# which data set to use
=======
# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, global, hourly_camels_us]
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
dataset: hourly_camels_us

# Path to data set root
data_dir: ../../data/CAMELS_US

<<<<<<< HEAD
# Forcing products used in the dataset.
# Ensure these align with the dynamic inputs you specify below.
=======
# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
# can be either a list of forcings or a single forcing product
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
forcings:
  - nldas_hourly
  - daymet

<<<<<<< HEAD
# Dynamic input variables by frequency.
# Ensure these lists contain the exact feature names as they appear in your dataset.
dynamic_inputs:
  1D:
    # Daymet daily forcings for 1D input stream
=======
dynamic_inputs:
  1D:
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
    - prcp(mm/day)_daymet
    - srad(W/m2)_daymet
    - tmax(C)_daymet
    - tmin(C)_daymet
    - vp(Pa)_daymet
  1h:
<<<<<<< HEAD
    # NLDAS hourly forcings
=======
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
    - convective_fraction_nldas_hourly
    - longwave_radiation_nldas_hourly
    - potential_energy_nldas_hourly
    - potential_evaporation_nldas_hourly
    - pressure_nldas_hourly
    - shortwave_radiation_nldas_hourly
    - specific_humidity_nldas_hourly
    - temperature_nldas_hourly
    - total_precipitation_nldas_hourly
    - wind_u_nldas_hourly
    - wind_v_nldas_hourly
<<<<<<< HEAD
    # Including Daymet daily forcings at 1h. These values will be repeated for each hour within a day.
=======
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
    - prcp(mm/day)_daymet
    - srad(W/m2)_daymet
    - tmax(C)_daymet
    - tmin(C)_daymet
    - vp(Pa)_daymet

# which columns to use as target
target_variables:
  - QObs(mm/h)

<<<<<<< HEAD
# clip negative predictions to zero for all variables listed below.
clip_targets_to_zero:
  - QObs(mm/h)
=======
# clip negative predictions to zero for all variables listed below. Should be a list, even for single variables.
clip_targets_to_zero:
  - QObs(mm/h)
>>>>>>> 4003273a910247b100337ec576cd4d8da85ed64f
