{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b41fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from neuralhydrology.evaluation import metrics\n",
    "from neuralhydrology.nh_run import start_run, eval_run\n",
    "from neuralhydrology.utils.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca222ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 01:26:53,358: Logging to d:\\github\\neuralhydrology\\neuralhydrology\\own\\LSTMAttention\\runs\\test_run_1105_012653\\output.log initialized.\n",
      "2025-05-11 01:26:53,358: ### Folder structure created at d:\\github\\neuralhydrology\\neuralhydrology\\own\\LSTMAttention\\runs\\test_run_1105_012653\n",
      "2025-05-11 01:26:53,358: ### Run configurations for test_run\n",
      "2025-05-11 01:26:53,358: experiment_name: test_run\n",
      "2025-05-11 01:26:53,368: train_basin_file: 1_basin.txt\n",
      "2025-05-11 01:26:53,368: validation_basin_file: 1_basin.txt\n",
      "2025-05-11 01:26:53,369: test_basin_file: 1_basin.txt\n",
      "2025-05-11 01:26:53,369: train_start_date: 1999-10-01 00:00:00\n",
      "2025-05-11 01:26:53,370: train_end_date: 2008-09-30 00:00:00\n",
      "2025-05-11 01:26:53,371: validation_start_date: 1980-10-01 00:00:00\n",
      "2025-05-11 01:26:53,372: validation_end_date: 1989-09-30 00:00:00\n",
      "2025-05-11 01:26:53,372: test_start_date: 1989-10-01 00:00:00\n",
      "2025-05-11 01:26:53,373: test_end_date: 1999-09-30 00:00:00\n",
      "2025-05-11 01:26:53,373: device: cpu\n",
      "2025-05-11 01:26:53,373: validate_every: 3\n",
      "2025-05-11 01:26:53,373: validate_n_random_basins: 1\n",
      "2025-05-11 01:26:53,373: metrics: ['NSE']\n",
      "2025-05-11 01:26:53,373: model: lstmattention\n",
      "2025-05-11 01:26:53,373: head: regression\n",
      "2025-05-11 01:26:53,373: output_activation: linear\n",
      "2025-05-11 01:26:53,373: hidden_size: 20\n",
      "2025-05-11 01:26:53,373: initial_forget_bias: 3\n",
      "2025-05-11 01:26:53,373: output_dropout: 0.4\n",
      "2025-05-11 01:26:53,373: optimizer: Adam\n",
      "2025-05-11 01:26:53,373: loss: MSE\n",
      "2025-05-11 01:26:53,373: learning_rate: {0: 0.01, 30: 0.005, 40: 0.001}\n",
      "2025-05-11 01:26:53,381: batch_size: 256\n",
      "2025-05-11 01:26:53,382: epochs: 50\n",
      "2025-05-11 01:26:53,382: clip_gradient_norm: 1\n",
      "2025-05-11 01:26:53,382: predict_last_n: 1\n",
      "2025-05-11 01:26:53,382: seq_length: 365\n",
      "2025-05-11 01:26:53,382: num_workers: 8\n",
      "2025-05-11 01:26:53,382: log_interval: 5\n",
      "2025-05-11 01:26:53,382: log_tensorboard: True\n",
      "2025-05-11 01:26:53,382: log_n_figures: 1\n",
      "2025-05-11 01:26:53,382: save_weights_every: 1\n",
      "2025-05-11 01:26:53,382: dataset: camels_us\n",
      "2025-05-11 01:26:53,382: data_dir: ..\\..\\data\\CAMELS_US\n",
      "2025-05-11 01:26:53,382: forcings: ['maurer', 'daymet', 'nldas']\n",
      "2025-05-11 01:26:53,382: dynamic_inputs: ['PRCP(mm/day)_nldas', 'PRCP(mm/day)_maurer', 'prcp(mm/day)_daymet', 'srad(W/m2)_daymet', 'tmax(C)_daymet', 'tmin(C)_daymet', 'vp(Pa)_daymet']\n",
      "2025-05-11 01:26:53,382: target_variables: ['QObs(mm/d)']\n",
      "2025-05-11 01:26:53,382: clip_targets_to_zero: ['QObs(mm/d)']\n",
      "2025-05-11 01:26:53,382: number_of_basins: 1\n",
      "2025-05-11 01:26:53,382: run_dir: d:\\github\\neuralhydrology\\neuralhydrology\\own\\LSTMAttention\\runs\\test_run_1105_012653\n",
      "2025-05-11 01:26:53,382: train_dir: d:\\github\\neuralhydrology\\neuralhydrology\\own\\LSTMAttention\\runs\\test_run_1105_012653\\train_data\n",
      "2025-05-11 01:26:53,382: img_log_dir: d:\\github\\neuralhydrology\\neuralhydrology\\own\\LSTMAttention\\runs\\test_run_1105_012653\\img_log\n",
      "2025-05-11 01:26:53,395: ### Device cpu will be used for training\n",
      "2025-05-11 01:26:53,395: Loading basin data into xarray data set.\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "2025-05-11 01:26:53,712: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "# Epoch 1: 100%|██████████| 13/13 [00:19<00:00,  1.50s/it, Loss: 0.4644]\n",
      "2025-05-11 01:27:16,425: Epoch 1 average loss: avg_loss: 0.47190, avg_total_loss: 0.47190\n",
      "# Epoch 2:   0%|          | 0/13 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m     start_run(config_file\u001b[38;5;241m=\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1_basin.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# fall back to CPU-only mode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1_basin.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\nh_run.py:77\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(config_file, gpu)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gpu \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     75\u001b[0m     config\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\training\\train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39minitialize_training()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\training\\basetrainer.py:214\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    212\u001b[0m         param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlearning_rate[epoch]\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m avg_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_logger\u001b[38;5;241m.\u001b[39msummarise()\n\u001b[0;32m    216\u001b[0m loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\training\\basetrainer.py:284\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Iterate in batches over training set\u001b[39;00m\n\u001b[0;32m    283\u001b[0m nan_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_updates_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_updates_per_epoch:\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# by default we assume that you have at least one CUDA-capable NVIDIA GPU or MacOS with Metal support\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    start_run(config_file=Path(\"1_basin.yml\"))\n",
    "\n",
    "# fall back to CPU-only mode\n",
    "else:\n",
    "    start_run(config_file=Path(\"1_basin.yml\"), gpu=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "850b6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial for config file: temp_configs\\config_attention_lr0p001_dp0p0.yml\n",
      "Learning Rate: 0.001, Dropout: 0.0\n",
      "2025-05-11 01:56:15,460: Logging to runs\\attention_lr0p001_dp0p0\\attention_lr0p001_dp0p0_1105_015615\\output.log initialized.\n",
      "2025-05-11 01:56:15,460: ### Folder structure created at runs\\attention_lr0p001_dp0p0\\attention_lr0p001_dp0p0_1105_015615\n",
      "2025-05-11 01:56:15,460: ### Run configurations for attention_lr0p001_dp0p0\n",
      "2025-05-11 01:56:15,460: batch_size: 256\n",
      "2025-05-11 01:56:15,460: clip_gradient_norm: 1\n",
      "2025-05-11 01:56:15,460: clip_targets_to_zero: ['QObs(mm/d)']\n",
      "2025-05-11 01:56:15,460: data_dir: ..\\..\\data\\CAMELS_US\n",
      "2025-05-11 01:56:15,460: dataset: camels_us\n",
      "2025-05-11 01:56:15,471: device: cpu\n",
      "2025-05-11 01:56:15,471: dynamic_inputs: ['PRCP(mm/day)_nldas', 'PRCP(mm/day)_maurer', 'prcp(mm/day)_daymet', 'srad(W/m2)_daymet', 'tmax(C)_daymet', 'tmin(C)_daymet', 'vp(Pa)_daymet']\n",
      "2025-05-11 01:56:15,472: epochs: 50\n",
      "2025-05-11 01:56:15,472: experiment_name: attention_lr0p001_dp0p0\n",
      "2025-05-11 01:56:15,473: forcings: ['maurer', 'daymet', 'nldas']\n",
      "2025-05-11 01:56:15,474: head: regression\n",
      "2025-05-11 01:56:15,474: hidden_size: 20\n",
      "2025-05-11 01:56:15,475: initial_forget_bias: 3\n",
      "2025-05-11 01:56:15,475: learning_rate: {0: 0.001}\n",
      "2025-05-11 01:56:15,476: log_interval: 5\n",
      "2025-05-11 01:56:15,477: log_n_figures: 1\n",
      "2025-05-11 01:56:15,478: log_tensorboard: True\n",
      "2025-05-11 01:56:15,479: loss: MSE\n",
      "2025-05-11 01:56:15,480: metrics: ['NSE']\n",
      "2025-05-11 01:56:15,480: model: lstmattention\n",
      "2025-05-11 01:56:15,480: num_workers: 8\n",
      "2025-05-11 01:56:15,481: optimizer: Adam\n",
      "2025-05-11 01:56:15,482: output_activation: linear\n",
      "2025-05-11 01:56:15,483: output_dropout: 0.0\n",
      "2025-05-11 01:56:15,484: predict_last_n: 1\n",
      "2025-05-11 01:56:15,486: run_dir: runs\\attention_lr0p001_dp0p0\\attention_lr0p001_dp0p0_1105_015615\n",
      "2025-05-11 01:56:15,487: save_weights_every: 1\n",
      "2025-05-11 01:56:15,487: seq_length: 365\n",
      "2025-05-11 01:56:15,487: target_variables: ['QObs(mm/d)']\n",
      "2025-05-11 01:56:15,487: test_basin_file: 1_basin.txt\n",
      "2025-05-11 01:56:15,487: test_end_date: 1999-09-30 00:00:00\n",
      "2025-05-11 01:56:15,487: test_start_date: 1989-10-01 00:00:00\n",
      "2025-05-11 01:56:15,487: train_basin_file: 1_basin.txt\n",
      "2025-05-11 01:56:15,487: train_end_date: 2008-09-30 00:00:00\n",
      "2025-05-11 01:56:15,487: train_start_date: 1999-10-01 00:00:00\n",
      "2025-05-11 01:56:15,487: validate_every: 3\n",
      "2025-05-11 01:56:15,487: validate_n_random_basins: 1\n",
      "2025-05-11 01:56:15,487: validation_basin_file: 1_basin.txt\n",
      "2025-05-11 01:56:15,487: validation_end_date: 1989-09-30 00:00:00\n",
      "2025-05-11 01:56:15,498: validation_start_date: 1980-10-01 00:00:00\n",
      "2025-05-11 01:56:15,499: number_of_basins: 1\n",
      "2025-05-11 01:56:15,499: train_dir: runs\\attention_lr0p001_dp0p0\\attention_lr0p001_dp0p0_1105_015615\\train_data\n",
      "2025-05-11 01:56:15,499: img_log_dir: runs\\attention_lr0p001_dp0p0\\attention_lr0p001_dp0p0_1105_015615\\img_log\n",
      "2025-05-11 01:56:15,506: ### Device cpu will be used for training\n",
      "2025-05-11 01:56:15,506: Loading basin data into xarray data set.\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
      "2025-05-11 01:56:15,657: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.15it/s]\n",
      "# Epoch 1: 100%|██████████| 13/13 [00:19<00:00,  1.49s/it, Loss: 0.6393]\n",
      "2025-05-11 01:56:35,130: Epoch 1 average loss: avg_loss: 0.50741, avg_total_loss: 0.50741\n",
      "# Epoch 2:   0%|          | 0/13 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Use CPU\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Start the NeuralHydrology run using the temporary config file path\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_config_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass the Path to the file\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\nh_run.py:77\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(config_file, gpu)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gpu \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     75\u001b[0m     config\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\training\\train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39minitialize_training()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\training\\basetrainer.py:214\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    212\u001b[0m         param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlearning_rate[epoch]\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m avg_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_logger\u001b[38;5;241m.\u001b[39msummarise()\n\u001b[0;32m    216\u001b[0m loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32md:\\github\\neuralhydrology\\neuralhydrology\\neuralhydrology\\training\\basetrainer.py:284\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Iterate in batches over training set\u001b[39;00m\n\u001b[0;32m    283\u001b[0m nan_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_updates_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_updates_per_epoch:\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Workstation\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import yaml # Need to import yaml to load/save dictionaries to/from file\n",
    "import os # Import os for creating directory if needed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# You might not need these imports if not directly using metrics or eval_run in this script\n",
    "# from neuralhydrology.evaluation import metrics\n",
    "from neuralhydrology.nh_run import start_run # Only need start_run\n",
    "# from neuralhydrology.utils.config import Config # Only need Config if you were interacting with the object directly\n",
    "\n",
    "# Define the values you want to test for hyperparameters\n",
    "learning_rates_to_test = [{0: 1e-3}, {0: 5e-4}, {0: 1e-4}] # Using dict format matching config\n",
    "dropouts_to_test = [0.0, 0.2, 0.4]\n",
    "\n",
    "# Define the path to your base configuration file\n",
    "base_config_path = Path(\"1_basin.yml\")\n",
    "\n",
    "# Define a directory to save temporary config files for each trial\n",
    "temp_config_dir = Path(\"temp_configs\")\n",
    "temp_config_dir.mkdir(exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# --- Load the base config into a dictionary ONCE outside the loop ---\n",
    "# This mutable dictionary will be our template for each trial\n",
    "with open(base_config_path, 'r') as f:\n",
    "    base_config_dict = yaml.safe_load(f)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning Loop ---\n",
    "# Loop through each combination of hyperparameters\n",
    "for lr_config in learning_rates_to_test:\n",
    "    for dropout_rate in dropouts_to_test:\n",
    "\n",
    "        # --- Create a MODIFIED dictionary for this trial ---\n",
    "        # Start with a copy of the base config dictionary for the current trial\n",
    "        trial_config_dict = base_config_dict.copy()\n",
    "\n",
    "        # Modify the dictionary with the current hyperparameters for this trial\n",
    "        trial_config_dict['learning_rate'] = lr_config\n",
    "        trial_config_dict['output_dropout'] = dropout_rate\n",
    "\n",
    "        # --- IMPORTANT: Set a UNIQUE experiment_name and run_dir for each trial ---\n",
    "        # This ensures each trial's results are saved in a separate folder\n",
    "        # Replace periods with 'p' to make directory names valid\n",
    "        exp_name = f\"attention_lr{str(lr_config[0]).replace('.', 'p')}_dp{str(dropout_rate).replace('.', 'p')}\"\n",
    "        trial_config_dict['experiment_name'] = exp_name\n",
    "        # Ensure the run directory is based on the unique experiment name\n",
    "        # You might need to adjust the base path for run_dir based on your setup\n",
    "        trial_config_dict['run_dir'] = str(Path(\"./runs\") / exp_name) # Example: ./runs/attention_lr0p001_dp0p0\n",
    "\n",
    "        # --- Save the modified dictionary to a temporary YAML file for this trial ---\n",
    "        # Create a unique filename for the temporary config\n",
    "        temp_config_filename = f\"config_{exp_name}.yml\"\n",
    "        temp_config_path = temp_config_dir / temp_config_filename\n",
    "\n",
    "        with open(temp_config_path, 'w') as f:\n",
    "            yaml.dump(trial_config_dict, f)\n",
    "        # -----------------------------------------------------------\n",
    "\n",
    "        # Print the config file being used for this trial (optional, for debugging)\n",
    "        print(f\"Starting trial for config file: {temp_config_path}\")\n",
    "        print(f\"Learning Rate: {lr_config[0]}, Dropout: {dropout_rate}\") # Print specific values\n",
    "\n",
    "        # Determine device based on availability\n",
    "        if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "            device = None # Use default device handling\n",
    "        else:\n",
    "            device = -1 # Use CPU\n",
    "\n",
    "        # Start the NeuralHydrology run using the temporary config file path\n",
    "        start_run(config_file=temp_config_path, gpu=device) # Pass the Path to the file\n",
    "\n",
    "        print(\"-\" * 30) # Separator between trials\n",
    "\n",
    "        # Optional: Clean up the temporary config file after the run\n",
    "        # import os\n",
    "        # os.remove(temp_config_path)\n",
    "\n",
    "\n",
    "# --- End of Hyperparameter Tuning Loop ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
